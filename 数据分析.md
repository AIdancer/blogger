索引
=========
[python去读ES数据,scroll](#python读取es)<br>
[ubuntu服务器](#ubuntu服务器)<br>
[ubuntu 源 bug](#ubuntu_source_bug解决)

偏置值
-----
  我们也需要加入一个额外的偏置量（bias），因为输入往往会带有一些无关的干扰量。

Elasticsearch content-type is not supported 错误:
------------------------------------------------
      To fix this, add curl option -H 'Content-Type: application/json'
      This error is due to strict content-type checking introduced in ElasticSearch 6.0, as explained in this post
      例如：curl -XPUT http://localhost:9200/blog/article/1 -d '{"title":"hello, world.", "tags":["bill", "bagens"]}' 
      -H 'Content-Type: application/json'
      
python读取es
--------------------------------------
```python
#encoding:utf8
from elasticsearch import Elasticsearch
from elasticsearch_dsl import connections
import requests
from elasticsearch_dsl.search import Search
import json

def test():
    res = requests.get('http://106.75.69.238:9200/logstash-nginx-access-finalversion01-2018-03/_search')
    print res.content

if __name__ == '__main__':
    print Elasticsearch.__doc__
    es = Elasticsearch(
            [
                'http://106.75.69.238:9200/'
            ]
        )
    print 'Connected', es.info()
    docs = es.get('logstash-nginx-access-finalversion01-2018-03', 'nginx-access-finalversion', 'AWIdcAJUFh8O2MxasDkf')
    print docs['_source'], docs['_source']['protocol']
    print docs['_source']['status']
    print 'Hello-----------------------'
    index_set = ['logstash-nginx-access-finalversion01-2018-03', 
                 'logstash-nginx-access-finalversion02-2018-03',
                 'logstash-nginx-access-finalversion03-2018-03']

    for idx in index_set:
        hits_list = []
        res = es.search(index=idx, body=
                  {
                    'size' : 1000,
                    'query' : {
                        'match_all' : {}
                    }
                   }, scroll='1m')
        total = totalhis = res['hits']['total']
        print 'totalhis :', totalhis
        while total-totalhis < 1000000:
            totalhis -= len(res['hits']['hits'])
            for hit in res['hits']['hits']:
                hits_list.append(hit)
            res = es.scroll(res['_scroll_id'], scroll='1m')
            print 'rate of process is : %.4f%%' % ((total - totalhis) * 100.0 / 1000000), total - totalhis
        print idx + 'finished'
        with open(idx+'.txt', 'w') as outfile:
                json.dump(hits_list, outfile)
    
```
  
查看java_home方法
----------------
    /usr/libexec/java_home -V
      
Hadoop笔记
-----------------
    HDFS的主要目标就是即使在出错的情况下也要保证数据存储的可靠性。常见的三种出错情况是：Namenode出错, Datanode出错和
    网络割裂(network partitions)。网络割裂的意思是指网络中有几个节点之间互不通信，最坏的情况下会产生两个Master。（其
    实还是不太懂，应该找本分布式的书看看）。
    
ubuntu服务器
-----------
    * ubuntu@172.22.16.157
    * hadoop user passwd : nichaichai
  
  
ubuntu_source_bug解决
---------------------
    http://blog.csdn.net/wang1144/article/details/51604424
